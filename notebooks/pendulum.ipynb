{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pilco.environments import Environment\n",
    "from pilco.policies import RBFPolicy\n",
    "from pilco.costs import EQCost\n",
    "from pilco.agents import EQGPAgent\n",
    "\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = tf.float64\n",
    "\n",
    "# Create pendulum environment from Gym\n",
    "env = Environment(name='Pendulum-v0')\n",
    "env.reset()\n",
    "\n",
    "# Create stuff for our controller\n",
    "# Upright position, stationary and zero action position for pendulum is [0, 0, 0]\n",
    "target_loc = tf.zeros([1, 2])\n",
    "target_scale = 1.\n",
    "\n",
    "eq_cost = EQCost(target_loc=target_loc,\n",
    "                 target_scale=target_scale,\n",
    "                 dtype=dtype)\n",
    "\n",
    "# create EQ policy\n",
    "eq_policy = RBFPolicy(state_dim=2,\n",
    "                      action_dim=1,\n",
    "                      num_rbf_features=5,\n",
    "                      dtype=dtype)\n",
    "\n",
    "# create agent\n",
    "eq_agent = EQGPAgent(state_dim=2,\n",
    "                     action_dim=1,\n",
    "                     policy=eq_policy,\n",
    "                     cost=eq_cost,\n",
    "                     dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96095b39e764673bcbfb0fc914c3b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state, action, next_state: [-2.01967675 -0.72846017], [-0.1438447], [-2.0909636  -1.42573701]\n",
      "state, action, next_state: [-2.0909636  -1.42573701], [-0.1438447], [-2.19586939 -2.09811575]\n",
      "state, action, next_state: [-2.19586939 -2.09811575], [0.06729627], [-2.33067997 -2.69621159]\n",
      "state, action, next_state: [-2.33067997 -2.69621159], [0.12072712], [-2.49176945 -3.22178965]\n",
      "state, action, next_state: [-2.49176945 -3.22178965], [0.09452371], [-2.67483922 -3.66139533]\n",
      "state, action, next_state: [-2.67483922 -3.66139533], [0.05734963], [-2.87435346 -3.99028491]\n",
      "state, action, next_state: [-2.87435346 -3.99028491], [0.03175944], [-3.08353212 -4.18357323]\n",
      "state, action, next_state: [-3.08353212 -4.18357323], [0.01818783], [-3.29475042 -4.22436599]\n",
      "state, action, next_state: [-3.29475042 -4.22436599], [0.01200873], [-3.50015767 -4.10814491]\n",
      "state, action, next_state: [-3.50015767 -4.10814491], [0.00982572], [-3.69233131 -3.8434729 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "num_steps = 10\n",
    "\n",
    "eq_agent.policy.reset()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    print(f\"Episode {episode + 1}\")\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in trange(num_steps):\n",
    "        \n",
    "        action = eq_agent.act(state)\n",
    "        state, action, next_state = env.step(action[None].numpy())\n",
    "        eq_agent.observe(state, action, next_state)\n",
    "        \n",
    "        #env.env.render()\n",
    "        \n",
    "        print(f\"state, action, next_state: {state}, {action}, {next_state}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 10\n",
    "\n",
    "init_state = tf.constant([[-np.pi, 0.]], dtype=tf.float64)\n",
    "init_cov = tf.eye(2, dtype=tf.float64)\n",
    "\n",
    "loc = init_state\n",
    "cov = init_cov\n",
    "\n",
    "cost = 0.\n",
    "\n",
    "for t in range(horizon):\n",
    "\n",
    "    mean_full, cov_full = eq_agent.policy.match_moments(loc, cov)\n",
    "\n",
    "    loc, cov = eq_agent.match_moments(mean_full, cov_full)\n",
    "\n",
    "    cost = cost + eq_agent.cost.expected_cost(loc[None, :], cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=6.448761337840721>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
